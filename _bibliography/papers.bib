

@article{mo2023test,
  title={Test-time backdoor mitigation for black-box large language models with defensive demonstrations},
  author={Mo, Wenjie and Xu, Jiashu and Liu, Qin and Wang, Jiongxiao and Yan, Jun and Xiao, Chaowei and Chen, Muhao},
  journal={arXiv preprint arXiv:2311.09763},
  year={2023},
  selected={true},
  abbr={preprint},
  pdf={https://arxiv.org/pdf/2311.09763},
  preview={test_time_defense.png},
}

@article{wang2024muirbench,
  title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
  author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and Yan, Tianyi Lorena and Mo, Wenjie Jacky and Liu, Hsiang-Hui and Lu, Pan and Li, Chunyuan and Xiao, Chaowei and Chang, Kai-Wei and Roth, Dan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2406.09411},
  year={2024},
  abbr={preprint},
  pdf={https://arxiv.org/pdf/2406.09411},
  preview={MuirBench.png},
}

@article{yan2024rethinking,
  title={Rethinking Backdoor Detection Evaluation for Language Models},
  author={Yan, Jun and Mo, Wenjie Jacky and Ren, Xiang and Jia, Robin},
  journal={arXiv preprint arXiv:2409.00399},
  year={2024},
  abbr={preprint},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2409.00399},
  preview={backdoor_detect.png},
}

@inproceedings{wang-etal-2023-causal,
    title = "A Causal View of Entity Bias in (Large) Language Models",
    author = "Wang, Fei  and
      Mo, Wenjie  and
      Wang, Yiwei  and
      Zhou, Wenxuan  and
      Chen, Muhao",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1013",
    doi = "10.18653/v1/2023.findings-emnlp.1013",
    pages = "15173--15184",
    abbr={ACL},
    pdf={https://aclanthology.org/2023.findings-emnlp.1013.pdf},
    preview={casual_view.png},
    abstract = "Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient semantic information from similar entities. Under the white-box setting, our training-time intervention improves OOD performance of PLMs on relation extraction (RE) and machine reading comprehension (MRC) by 5.7 points and by 9.1 points, respectively. Under the black-box setting, our in-context intervention effectively reduces the entity-based knowledge conflicts of GPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on MRC and up to 17.6 points of reduction in memorization ratio on RE.",
}
